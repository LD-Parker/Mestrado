1
Enhancing Generalizability of Predictive Models
with Synergy of Data and Physics
Yingjun Shen, Zhe Song,Member, IEEE , Andrew Kusiak, Life Member, IEEE
Abstract —Wind farm needs prediction models for predictive
maintenance. There is a need to predict values of non-observable
parameters beyond ranges reﬂected in available data. A pre-
diction model developed for one machine many not perform
well in another similar machine. This is usually due to lack
of generalizability of data-driven models. To increase general-
izability of predictive models, this research integrates the data
mining with ﬁrst-principle knowledge. Physics-based principles
are combined with machine learning algorithms through feature
engineering, strong rules and divide-and-conquer. The proposed
synergy concept is illustrated with the wind turbine blade icing
prediction and achieves signiﬁcant prediction accuracy across
different turbines. The proposed process is widely accepted by
wind energy predictive maintenance practitioners because of its
simplicity and efﬁciency. Furthermore, this paper demonstrates
the importance of embedding physical principles within the
machine learning process, and also highlight an important point
that the need for more complex machine learning algorithms in
industrial big data mining is often much less than it is in other
applications, making it essential to incorporate physics and follow
“Less is More” philosophy.
Index Terms —Industrial big data, machine learning, process
engineering, physical principle, prediction model, wind turbine
blade icing.
I. I NTRODUCTION
Manufacturing becomes more and more intelligent with
the development of Big Data, Cloud Computing, Internet of
Things, and Artiﬁcial Intelligence [1]. Sensors are installed
to collect data at all stages of production, such as material
properties, temperatures and vibrations of equipments and
customer characteristics [2], [3]. The digital manufacturing
transformation has led to more sensors generating large vol-
umes of data which can be mined for many purposes. For
example, engineers can improve product quality, optimize ex-
isting processes, predict emerging failures based on historical
data [4]–[8].
How to build generalizable predictive models is one of
the major challenges in machine learning research. Predictive
models should be able to handle uncertainties and generalize
before industrial deployment [1], [9]. Prediction models in
industrial ﬁelds are often characterized as highly nonlinear
and high-dimensional.
Corresponding author: Zhe Song.
Y . Shen is with the Business School, Nanjing University, Nanjing, 210093,
China (e-mail: dg1902054@smail.nju.edu.cn).
Z. Song is with the Business School, Nanjing University, Nanjing, 210093,
China. (e-mail: zsong1@ nju.edu.cn).
A. Kusiak is with the Department of Industrial and Systems Engineering,
4627 Seamans Center for the Engineering Arts and Sciences, The University
of Iowa, Iowa City, IA, USA (e-mail: andrew-kusiak@uiowa.edu).Generally speaking, there are two ways of building predic-
tion models for complex industrial systems [10]. One is top-
down approach. The other is bottom-up approach. Top-down
approach usually refers to constructing models by existing
knowledge, such as Newton’s laws, aerodynamics, chemical
reactions and so on. Top-down approach is also known as
white-box model, or mechanistic model which is explainable,
understandable and generalizable. Mechanistic model usually
can’t predict well in high-resolution because of “random
noises” in reality. Sometimes engineers work with problems
for which there is no simple or well-understood mecha-
nistic model that explains the phenomenon. Pure physics-
based model will fall short in practical industrial setting. For
example, Betz law may tell you the exact relationship between
wind speed and the energy captured by a wind turbine. But
100 hundred same-type wind turbines will show 100 hundred
different power curves drawn from ﬁeld data [11].
Bottom-up approach is usually related with data-driven
methods. Models are derived from data by data mining or
machine learning algorithms, which are sometimes called
black boxes or empirical models [12], [13]. For example,
artiﬁcial neural networks are used to predict gearbox failures.
Deep learning algorithms are used to predict wind turbine
blade breakage [14], [15]. Not like mechanistic model, data-
driven empirical model may not generalize, but can make
accurate predictions locally (somehow memorize the data you
feed into the learning algorithm). As you can imagine, a
stable production system maintains the controllable process
variables, such as temperatures, as closely as possible to
desired targets or set points. Because the controllable variables
change so little, it may be difﬁcult to assess their real impact
on target response variables. On the other hand, in industrial
scenario, data mining may involve a lot of data, but that data
may contain relatively little useful information about the whole
picture of the problem. Data is usually unbalanced because
system runs in several steady states. Furthermore, some of
the relevant data may be missing, there may be transcription
or recording errors resulting in outliers, or data on other
important factors may not have been collected and archived.
In reality, it happens to predict beyond speciﬁc conditions
where data may be available. For these reasons, a purely data-
driven prediction model won’t work.
Achieving generalizable predictive models for these com-
plex systems requires a synergistic combination of data
and physics-based knowledge [16], [17]. Learning from data
through the lens of physical principle is a way to bring gen-
eralizability to an otherwise intractable prediction modeling
problem. It is a way to integrate physical laws and domainarXiv:2105.01429v1  [cs.LG]  4 May 2021
------------------------------End of the page -----------------------------------
2
DataBusiness UnderstandingData PreparationData Understanding
ModelingValidation and EvaluationDeploy
Fig. 1. CRISP-DM process
knowledge. So, one promising direction is hybrid modeling.
But how to embed the knowledge into the modeling process
is an open research question.
Although there are hundreds of different machine learning
algorithms available to build predictive models, few algorithms
are designed speciﬁcally for improving model generalizability.
In other words, algorithms alone are hard-pressed to learn a
generalizable predictive model. However, if machine learning
is viewed as a production process and ﬁnal products are
predictive models, new research efforts could be focused on
optimizing the learning process. By scrutinizing the process
and ﬁxing potential loopholes, it is possible to wringing every
last drop of value from industrial big data [18]–[21].
Generally speaking, machine learning process can be di-
vided into several parts: business understanding and data
preparation, feature engineering, machine learning algorithm
selection or parameter tuning, training, validation and testing,
model deployment. Cross-Industry Standard Process for Data
Mining (CRISP-DM) is a standard data mining process ini-
tiated in 1999, which is adopted by industry and academia
for nearly 20 years with little innovation (Fig. 1) . Recent
research about data mining and machine learning for predictive
modeling is usually focused on algorithms innovation, such as
deep learning algorithms [15].
In existing literatures, predictive models are extracted from
data by supervised learning algorithms. Generalizability is
considered by choosing simpler models or increasing the
data set size. Speciﬁc examples are early stopping or post-
pruning of decision tree algorithms [12], [22]. In this paper,
generalizability of predictive models is improved through
process renovation. Just like ImageNet solved the image
recognition accuracy problem not by creating new machine
learning algorithms, but by growing high-quality labeled im-
age database. Physical principle is embedded into the learning
process and new sub-processes are created to improve the
predictive model’s generalizability. Our contribution is not
creating new machine learning algorithms, but enriching thetraditional data mining process. Sub-processes are developed
to integrate physical principles, such as generating useful
features according to physical equations, dividing machine
learning problem into sub-problems. Good features based
on physical principles will signiﬁcantly improve prediction
model’s generalizability. Physical principles could guide us to
divide the prediction model into several simpler sub-models,
which could be learnt easily. This paper used real-world wind
turbine blade icing prediction to validate the proposed new
process.
This article is structured as follows: case study background
and data description are presented in Section II. Methods
and processes are introduced in Section III. Section IV and
Section V describes the computational processes, experiments
and competition results. Implications and limitations of the
study are discussed in Section VI.
II. C ASE STUDY BACKGROUND AND DATA DESCRIPTION
Case study of this paper is based on China Industrial Big
Data Innovation Competition in 2017 , which is the ﬁrst author-
itative national competition in the ﬁeld of industrial big data
under the guidance of Ministry of Industry and Information
Technology. The competition’s goal is to help companies solve
realistic industrial big data problems, improve the level of
smart manufacturing. The competition problem is to build
blade icing prediction model for Wind Turbines (WT) from
historical Supervisory Control and Data Acquisition (SCADA)
data. The authors participated in the competition and achieved
the second place out of nearly 1500 teams. The competition
is based on blind tests which has proven the effectiveness of
our method.
Predictive maintenance and prognosis health management
with SCADA data is the prevailing strategy adopted by wind
farm operators [8], [23], [24]. Prediction model is the key
technology for wind power industry [11], [25]. Wind turbine’s
tower height is increasing with the rated power output [26]. In
China northern coastal or mountainous areas, a large number
of wind turbines will touch the lower clouds in winter, which
is very easy to freeze in the low-temperature and humid
environment. Blade icing is a great threat to wind power
generation and safe operation. Experience shows that blade
icing will change blade shape and undermine aerodynamic
characteristics, resulting in the lower efﬁciency and unstable
production [27]–[29]. Predicting blade icing is to compare
a turbine’s SCADA power curve with the theoretic one.
If the SCADA power curve is signiﬁcantly lower than the
theoretic one, alarm is triggered and wind turbine is shut down.
However, this approach isn’t working well in reality. Alarm is
usually triggered too late and large area icing is already formed
on the blades. The blades will suffer high risk of breakage and
may cause disastrous accidents. Thus, it is necessary to have
an accurate blade icing prediction model so that the alarm can
be triggered in the early stage of icing and de-icing system is
turned on.
The competition organizer provided 6-month SCADA data
of ﬁve wind turbines (named as WTs 8, 10, 14, 15, and 21).
Each wind turbine has 28 variables such as wind speed, power
------------------------------End of the page -----------------------------------
3
TABLE I
SCADA DATA SET VARIABLES (FEATURES )
Variables Description
time Time stamp
wind speed Wind speed
generator speed Generator speed
power Active power on network side
wind direction Opposite wind angle
wind direction mean 25 seconds average wind direction
yaw position Yaw position
yaw speed Yaw speed
pitch1 angle Blade 1 angle
pitch2 angle Blade 2 angle
pitch3 angle Blade 3 angle
pitch1 speed Blade 1 speed
pitch2 speed Blade 2 speed
pitch3 speed Blade 3 speed
pitch1 moto tmp Pitch motor 1 temperature
pitch2 moto tmp Pitch motor 2 temperature
pitch3 moto tmp Pitch motor 3 temperature
accx X-direction acceleration
accy Y-direction acceleration
environment tmp Ambient temperature
inttmp Cabin temperature
pitch1 ng5 tmp Charger 1 temperature
pitch2 ng5 tmp Charger 2 temperature
pitch3 ng5 tmp Charger 3 temperature
pitch1 ng5 DC Current of charger 1
pitch2 ng5 DC Current of charger 2
pitch3 ng5 DC Current of charger 3
group Data grouping identiﬁcation
and environmental temperature, which are listed in Table I.
Historical SCADA data of WTs 15 and 21 are used for
training. Icing and no-icing time periods are provided so that
“normal” and “abnormal” tags could be labeled for WTs 15
and 21. Based on the labeled data, supervised machine learning
algorithms are applied to build prediction models.
WTs 8, 10 and14 are used for blind testing, where the icing
and no-icing information is not provided in the data sets. WT
8 is used for preliminary testing and contestants can submit
their prediction results on a daily basis. The organizer will
calculate and rank prediction scores based on the submitted
predictions. Contestants can use this score to evaluate their
algorithms’ performance and make necessary adjustments.
WTs 10 and 14 are used for ﬁnal testing, which is released
only two days before ﬁnal submission. Contestants will have
only two chances of submitting predictions in two days. The
ﬁnal prediction score is based on WTs 10 and 14’s prediction
accuracy.
The total datasets are composed of 1,124,741 observations
collected between November 2015 and January 2016, where
584,380 observations are labeled (WTs 15 and 21). Table II
shows details of 5 wind turbine SCADA datasets.
.
III. D ATAMINING PROCESS RENOVATION WITH PHYSICAL
PRINCIPLE
“Less is More” philosophy and Occam’s Razor theorem
tell us to choose simple yet accurate model for engineer-
ing applications [12], [30]. In order to build generalizable
prediction model, it is necessary to renovate existing dataTABLE II
DESCRIPTION OF COMPETITION DATASETS
Wind turbine Total observations Icing observations Sampling frequency (sec-
onds)
15 393886 23892 7,8,10
21 190494 10638 7,8,10
8 202328 Not disclosed Not disclosed
10 174301 Not disclosed Not disclosed
14 163732 Not disclosed Not disclosed
Training data
Strong rule ﬁlteringPreprocessingFeature engineering
Model testing for sub-problemsAccuracy and generalization performanceData segementation
DeploymentModel training for sub-problems• Labeling• De-noising• Balancing• Feature generation• Feature transformation• Feature selectionPhysical principle“if … then … ” rules are derived based on domain knowledgePhysical principleDividing total training data into subsets according to the statesPhysical principle• Machine learning algorithm selection• Cross validation
AcceptableNot acceptable( sub-problems )
Fig. 2. Machine learning process for complex engineering problems with
physical principle integration
mining process by utilizing physical principle (see Fig. 2,
three highlighted sub-processes). Previous research has shown
that feature engineering is one of the most important machine
learning process. Applied and real-world machine learning is
basically feature engineering [22], [31]. It is a perfect spot to
embed physical principle into feature engineering.
For complex industrial big data mining, “Divide and Con-
quer” strategy is good at building more accurate and simpler
prediction models. As most industrial systems are running in
a set of different states, it is very reasonable to divide total
training data into subsets according to these states. Then for
each subset of training data, applying appropriate machine
------------------------------End of the page -----------------------------------
4
TABLE III
DISTRIBUTION OF LABELED DATASETS
Dataset abnormal normal invalid
WT 15 23892 350255 19739
WT 21 10638 168930 10926
learning algorithm to learn the prediction model. As a result,
it is natural to use different prediction models for different
states respectively, which will have a better prediction per-
formance than building only one prediction model. Following
the “Divide and Conquer” strategy, it is necessary to build
separate blade icing prediction models for low wind speed
and high wind speed scenarios. It is noteworthy that “Divide
and Conquer” strategy may fail if there are not enough training
samples as you try to partition the training data set into too
many categories.
Sometimes observations of training data set can be well
explained or predicted by physical principles. In such circum-
stances, there is no need to use machine learning algorithm for
prediction. Physical principle can be transformed into a set of
“IF. . . THEN. . . ” rules. For example, IF outside temperature
is above some number, THEN it is impossible to have icing
problem. IF wind speed is above some value AND power
is above some value, THEN there is no blade icing. These
rules could be called strong rules. Strong-rule ﬁltering is
a necessary process before letting machine learn. Machine
learning algorithms are focused on data points which are not
well explained by physical principle or domain knowledge.
Based on above discussions, traditional machine learning
process is re-engineered to incorporate physical principle.
Fig 2 is the new machine learning process where feature
engineering guided by physical principle, strong rule ﬁltering
and data segmentation (“Divide and Conquer”) are added. Like
the CRISP-DM process, this reengineered process is not a pure
cascade, but allow renewal and iteration if the ﬁnal data-driven
prediction models don’t perform well.
IV. C OMPUTATIONAL PROCESS
A. Data Preprocessing
Icing and no-icing time periods are provided so that “nor-
mal” and “abnormal” tags are labeled for WTs 15 and 21.
Table III listed the number of observations for each cate-
gory. “invalid” means that the observations don’t belong to
“normal” or “abnormal”, which could be discarded. For WT
15, there 23892 abnormal samples, 350255 normal samples,
19739 invalid samples. Similar pattern is shown for WT 21.
It is obvious to see that the training data set is extremely
unbalanced. Normal samples occupied most part of the data
set. Under-sampling technique is used for preprocessing and
make the training data set evenly distributed across different
categories [32], [33]. As the invalid samples don’t contain
useful information for icing prediction and only accounts for
0.55% of the total samples, they are deleted.
Another important task of data preprocessing is denoising.
In industrial application, data collected by various sensors
are usually noisy due to high-frequency sampling and sensor
Fig. 3. Density distribution of raw features for normal and abnormal samples
malfunctions. Wind turbine SCADA data used in this paper is
collected every 7 seconds. But not all sensors, such as wind
speed sensor, have such high precision. Therefore, moving
average can smooth the original time series. Different steps (5,
10, 15, 25) of moving average are tried to denoise the training
data. This paper ﬁnally selected 10-step moving average based
on try and error. Besides moving average denoising, other
sophisticated denoising methods, such as wavelet, could be
used as well. But in this paper, it is not the focus to discuss
what is the best way of denoising.
B. Feature Engineering
Feature engineering is essential to applied machine learning
problems. A good feature is worth a thousand words. By
observing density distributions of original features between
normal and abnormal samples, there are some features, such
as wind speed, generator speed, power and pitch motor tem-
perature, show observable differences between normal and
abnormal data sets (see Fig 3). Some features are not so dis-
tinguishing in terms of icing and no-icing, such as yaw speed
and acceleration, which may not have effective classiﬁcation
power. The goal of feature engineering by physical principle
is to generate new explainable and distinguishing features
Mainstream wind turbine has three blades which should be
theoretically controlled simultaneously. In the original SCADA
data, each blade is equipped with sensors to monitor pitch
angle, pitch speed, motor temperature and motor electrical
------------------------------End of the page -----------------------------------
5
Fig. 4. Scatter plot of wind speed and generator speed under normal and
abnormal conditions
current and so on. These monitored values are basically same
across three blades. So, the average values of these features
may better reﬂect the overall blades status than using original
features independently. New features such as average pitch
angle, average pitch speed, average pitch motor temperature,
and so on, are generated. Temperature difference between
inside and outside is generated because it is a good indicator
of icing. These new features are listed in Table IV.
Wind turbine blade icing is a typical physical phenomenon.
Icing will change the shape of blades and thus undermine the
aerodynamic properties [28], [34]. Icing is a slow physical
process where the energy is accumulated over time. Wind
speed, outside temperature, humidity, wind turbine height,
blade shape, rated power, rotational speed and so on, all could
inﬂuence when and where the icing starts. Based on physical
principle, original features are transformed into new ones,
such as torque, power coefﬁcient, thrust coefﬁcient and tip-
speed ratio. Theoretically, these new features should show
good distinguishing capability. The calculation methods of
these new features are show in Table V. Because the original
data is desensitization, it is necessary to add 5 to avoid 0 in the
denominator. Note that the formula used here is not exactly the
same as the one speciﬁed in physical textbook because some
required coefﬁcients or variables are not available or can’t be
directly measured.
Fig 4 and Fig 5 show scatter plots of wind speed and other
features, different patterns are obvious between normal and
abnormal samples. Thus, the feature transformation based on
physical principle is effective.
The new feature and original ones are further selected based
on classical feature selection algorithm to reduce prediction
model’s complexity. Final selected features are show in Ta-
ble VI. The density distributions of these features under normal
and abnormal conditions are shown in Fig 6.
C. Strong Rule Filtering
Based on the preliminary data exploration, blade icing
mainly occurs at low wind speed, low power, low temperature
and small blade angle, which can be observed in Fig 4 and
Fig. 5. Scatter plot of wind speed and temperature difference of normal and
abnormal samples
Fig. 6. Density distribution of selected features under normal and abnormal
conditions
Fig 5. Based on domain knowledge and physical principle, 5
rules could be derived and tell us under what conditions icing
is going to happen. These 5 rules could effectively reduce the
learning time by excluding noisy data and samples that are
obviously not icing. So, the machine learning algorithm could
focus on learning from data satisfying these rules.
Rule 1: x4<2; (IF wind speed is smaller than 2 THEN
icing is possible);
------------------------------End of the page -----------------------------------
6
TABLE IV
FEATURE GENERATED BASED ON SIMPLE STATISTICAL TRANSFORMATION AND DOMAIN KNOWLEDGE
New features Description Formula
pitch angle Ave average pitch angle pitch angle Ave =(pitch1 angle+pitch2 angle+pitch3 angle)
3
pitch speed Ave average pitch speed pitch speed Ave =(pitch1 speed+pitch2 speed+pitch3 speed)
3
pitch moto tmpAve average pitch motor temperature pitch moto tmpAve =(pitch1 moto tmp+pitch2 moto tmp+pitch3 moto tmp)
3
pitch ng5 tmpAve average pitch Ng5 temperature pitch ng5 tmpAve =(pitch1 ng5 tmp+pitch2 ng5 tmp+pitch3 ng5 tmp)
3
pitch ng5 DCAve average pitch Ng5 DC pitch ng5 DCAve =(pitch1 ng5 DC+pitch2 ng5 DC+pitch3 ng5 DC)
3
tmp diff difference between inside and outside tem-
peraturetmp diff = int tmp environment tmp
TABLE V
NEW FEATURE GENERATED BASED ON PHYSICAL PRINCIPLE
New features Description Formula
torque torque torque=(power +5)
(generator speed+5)
Cp power factor Cp=(power +5)
((wind speed+5)3)
Ct thrust coefﬁcient Ct =torque
(wind speed+5)2
 tip-speed ratio =(generator speed+5)
(wind speed+5)
TABLE VI
FINAL SELECTED FEATURES TO BUILD PREDICTION MODEL OF BLADE
ICING
Variable Feature Description
x1pitch1 moto tmp pitch motor 1 temperature
x2pitch2 moto tmp pitch motor 2 temperature
x3 pitch3 moto tmp pitch motor 3 temperature
x4wind speed wind speed
x5 environment tmp outside temperature
x6 tmp diff temperature difference between inside and outside
x7power power
x8  tip-speed ratio
x9torque torque
x10pitch angle Ave average pitch angle
y icing label normal or abnormal
Rule 2: 0:2x100:4; (IF average pitch angle is
between 0.2 and 0.4 THEN icing is possible);
Rule 3: x4<2 & 0 :2x100:4; (IF wind speed is
smaller than 2, and average pitch angle is between 0.2
and 0.4 THEN icing is possible);
Rule 4: x4<2 &x5<1:5 & 0 :15< x 10<0:36; (IF
wind speed is smaller than 2, and average pitch angle is
between 0.15 and 0.36 THEN icing is possible);
Rule 5: x4<2 &x5<1:5 & 0 :15< x 10<0:36 & x7<
2; (IF wind speed smaller than 2, and outside temperature
is less than 1.5, and average pitch angle is between
0.15 and 0.36, and power is less than 2 THEN icing is
possible).
After several groups of try and error experiments, Rule 5 is
chosen as the ﬁnal strong rule for ﬁltering.
Observations outside the strong rule are normal (no-icing)
data. The strong rule visualization results of the training data
are shown in Fig 7. It can be observed that the outside
environment temperature and wind speed are in a certain range
when blade is no-icing, see bottom part of Fig 7. Fig 7’s
middle part shows the power curves, i.e. scatter plot of wind
speed and power, exhibit obvious patterns between icing and
no-icing. When wind speed is high enough and power is large
Fig. 7. Strong rule data visualization
enough, the chance of blades getting frozen is little. Upper
part of Fig 7 (scatter plot of wind speed and average pitch
angle) tells us that icing usually happened when blade pitch
angle is controlled at a small value. The underlying reason is
that only when wind speed is large enough, control system is
activated to pitch blades to cast off extra wind energy.
D. Data Segmentation
As natural wind, the fuel, is not controllable, wind turbine
is a complex energy conversion system and operated under
different states according to predeﬁned requirements. If wind
speed is too high, larger than the cut-out speed, wind turbine
will be shut down to keep safety. Cut-in wind speed is selected
as the segmentation point based on physical principle. Below
and above the cut-in wind speed, the operational logic of
wind turbine is theoretically different. As the training data
was desensitization, the exact cut-in wind speed is not clear.
Segmentation points are mostly negative and 0, -0.25, -0.5,
-0.5, -0.75, -1 are tried to segment the data into two subsets,
one is higher than cut-in wind speed and the other is lower
than cut-in wind speed. After several rounds of experiments
the optimal segmentation point is determined at -0.25.
V. C OMPUTATIONAL AND COMPETITION RESULTS
Table III shows the training sample distribution before
balancing treatment. Using unbalanced training data set to
construct prediction model will result in poor performance.
Over-sampling, under-sampling and hybrid sampling are tried
to equalize the training data. Computational experiments show
that the optimal equalization method is under-sampling, i.e. to
------------------------------End of the page -----------------------------------
7
TABLE VII
DESCRIPTION OF TP, TN, FN AND FP
Predict
Normal Fault
ActualNormal True Positive (TP) False Negative (FN)
Fault False Positive (FP) True Negative (TN)
TABLE VIII
SCORES OF TRADITIONAL MACHINE LEARNING PROCESS WITHOUT
PHYSICAL PRINCIPLE INTEGRATION
KNN(k=3) CART DNN
Mean Std. Mean Std. Mean Std.
5-fold-cross-
validation90.98 0.44 89.91 0.89 85.21 1.07
Train: WT 15 Test: WT 21
Test score 67.45 0.43 62.34 7.96 75.96 2.57
5-fold-cross-
validation89.75 0.70 95.18 0.62 86.17 1.43
Train: WT 21 Test: WT 15
Test score 57.06 0.50 62.10 2.36 72.87 2.61
randomly sample data from normal data points and put them
together with icing data points to form a new training data set.
Three machine-learning classiﬁcation algorithms: K Near-
est Neighbors (KNN), Classiﬁcation And Regression Tree
(CART), and Deep Neural Network (DNN) are used for
modeling. TP, TN, FN and FP (see Table VII) are calculated
respectively based on testing data set and cross-validation. The
scores of different algorithms are calculated by Equation 1,
where Nnormal andNfaultstand for the number of no-icing and
icing data samples in the testing data set
Score =
1 0:5FN
Nnormal 0:5FP
Nfault
100 (1)
Two sets of comparative experiments are conducted to verify
the generalizability of the re-engineered machine learning
process (Table VIII and Table IX). WT 15 is for training
and WT 21 is for testing, and vice versa (Fig 8, Highlighted
diamond indicates whether the training is based on the new
sub-processes or not). Learning prediction models from one
turbine and test the models on another turbine is going to give
the generalization performance. In order to ensure reliability
of experimental results, this paper conducted ten repeated
experiments, prediction models are trained on the data of ten
random under-sampling, the cross-validation scores and test
scores on the other wind turbine are relatively stable. The
mean score and its 3standard deviation of 10 experiments with
traditional machine learning process are show in Table VIII.
Three classical machine learning algorithms all obtained very
good 5-fold cross-validation training scores. However, their
test scores are very low and not acceptable for practical de-
ployment, which means that training a prediction model from
WT 15 won’t work for WT 21, and vice versa. High training
scores can’t guarantee the prediction model’s generalization
performance due to overﬁtting, multiple wind turbine running
states and lack of strong features.
The mean score and its standard deviation of 10 experiments
with new machine learning process are show in Table IX.
Wind turbine B
ModelingFive-fold cross validation scoreTestTest scoreSub-processes based on physical principleYesModelingNoFive-fold validation scoreWind turbine AModelingFive-fold cross validation scoreTestTest score
Sub-processes based on physical principleModelingNoYesFive-fold validation scoreFig. 8. Computational experiments for comparing re-engineered machine
learning process with traditional one
TABLE IX
SCORES OF RE -ENGINEERED MACHINE LEARNING PROCESS WITH
PHYSICAL PRINCIPLE INTEGRATION
KNN(k=3) CART DNN
Mean Std. Mean Std. Mean Std.
Train: WT 15 Test: WT 21Low speed5-fold cross
validation93.40 5.57 87.64 1.61 84.81 2.98
Test score 77.72 1.39 83.26 1.68 76.84 6.62
High speed5-fold cross
validation96.92 0.39 89.76 0.82 85.97 0.88
Test score 84.21 0.70 73.19 2.85 78.39 4.66
Train: WT 21 Test: WT 15Low speed5-fold cross
validation96.81 0.42 87.84 0.76 86.30 1.46
Test score 96.70 0.23 85.91 0.83 84.47 1.77
High speed5-fold cross
validation97.03 0.33 89.47 0.45 85.95 0.94
Test score 96.78 0.24 87.93 0.83 84.72 2.15
Compared with Table VIII, the new sub-processes based on
physical principle can signiﬁcantly improve the generalizabil-
ity and accuracy of prediction models. “Data Segmentation”
allows the machine learning algorithm to learn simple yet
accurate models for different running states; “Strong Rule
Filtering” prevents the machine learning algorithm overﬁtting
noisy or “common-sense” samples; “Feature Engineering”
generate powerful and explainable features for the machine
learning algorithm.
------------------------------End of the page -----------------------------------
8
TABLE X
PRELIMINARY AND FINAL SCORES WITH RANKINGS
Blind Test Dataset Score Algorithm Ranking
Preliminary WT 8 89.23 KNN (K=3) 2
Final WT 10, WT 14 82.01 KNN (K=3) 2
During the competition, to ensure the generalizability of
ﬁnal prediction model, different random seeds are used to
conduct repeated experiments, and selected the most stable
model with highest prediction score, then it is deployed to
predict the ﬁnal test data set provided by the competition
organizer. Through previous training and testing, the KNN
model has highest prediction ability and generalize very
well. Therefore, it is chosen as the prediction model for
the preliminary testing and ﬁnal competition. In the ﬁnal
blind test, our submitted results ranked the second place.
The scores of the preliminary and ﬁnal tests are shown in
Table X. The prediction model used by the champion team
is based on CNN-LSTM (Convolutional Neural Network and
Long Short-Term Memory Neural Network), a deep learning
algorithm, their score is 82.54, which is slightly better than our
KNN’s score 82.01. However, during the ﬁnal presentation in
Beijing, our method is well recognized by competition judges,
domain experts and industrial practitioners in terms of sim-
plicity, computing efﬁciency and generalizability. As a result,
our presentation score is the number one. Total runtime is
about 555.6 seconds, including raw data preprocessing 160.34
seconds, feature engineering 384.72 seconds, model training
9.06 seconds, new data set testing 2.24 seconds. Running
environment is 64bit-Win10, CPU i7-2600 3.4GHz, RAM 8G.
The R code (448 lines) is complied with R version 64bit-
3.3.2. Other contestants, i.e., the ﬁrst-place team with deep
learning algorithms spent more than 6 hours on preprocessing
and training with similar computers.
VI. CONCLUSION
Like manufacturing an industrial equipment, this paper
treats machine learning as a production process and ﬁnal prod-
uct is prediction model. In order to improve prediction model’s
generalizability, this paper re-engineered the traditional ma-
chine learning process by integrating physical principle. Com-
putational experiments and blind-test competition based on
real-world industrial data sets prove the effectiveness of our
methodology. This paper also shows that simple machine
learning algorithms can compete with deep learning algorithms
with the new process in industrial setting, which may shed
light on an even bigger research question, is the learning
process important or the learning algorithm important? Re-
cent fast development of deep learning algorithms and their
successful applications in image or speech recognition are in
deep contrast with limited predictive capabilities in industrial
ﬁelds. It is time to rethink machine learning in a systematic
way, where high-quality data, algorithms, processes and later
maintenance are all essential to successful deployment of in-
dustrial data-driven prediction models. However, this research
is limited in covering depth and width of machine learningprocess engineering. Other sub-processes and life-cycle of
prediction model management are not discussed, and could be
our future research directions. Data-driven predictive models
for different industries, different equipments and systems may
need different machine learning processes, which is not fully
researched in this paper and worthy of further investigation.
REFERENCES
[1] A. Kusiak, “Smart manufacturing must embrace big data,” Nature News ,
vol. 544, no. 7648, p. 23, 2017.
[2] S. Jeschke, C. Brecher, T. Meisen, D. ¨Ozdemir, and T. Eschert, “Indus-
trial internet of things and cyber manufacturing systems,” in Industrial
internet of things . Springer, 2017, pp. 3–19.
[3] L. Da Xu, W. He, and S. Li, “Internet of things in industries: A survey,”
IEEE Transactions on industrial informatics , vol. 10, no. 4, pp. 2233–
2243, 2014.
[4] G. Helbing and M. Ritter, “Deep learning for fault detection in wind
turbines,” Renewable and Sustainable Energy Reviews , vol. 98, pp. 189–
198, 2018.
[5] A. Kusiak, “Big data in mechanical engineering,” ME Today , no. March,
2015.
[6] ——, “Break through with big data,” Industrial Engineer , vol. 47, no. 3,
pp. 38–42, 2015.
[7] A. McAfee, E. Brynjolfsson, T. H. Davenport, D. Patil, and D. Bar-
ton, “Big data: the management revolution,” Harvard business review ,
vol. 90, no. 10, pp. 60–68, 2012.
[8] A. Stetco, F. Dinmohammadi, X. Zhao, V . Robu, D. Flynn, M. Barnes,
J. Keane, and G. Nenadic, “Machine learning methods for wind turbine
condition monitoring: A review,” Renewable energy , vol. 133, pp. 620–
635, 2019.
[9] A. Kusiak, “A four-part plan for smart manufacturing,” ISE Magazine ,
vol. 49, no. 7, pp. 43–47, 2017.
[10] Y .-H. Kuo and A. Kusiak, “From data to big data in production
research: the past and future trends,” International Journal of Production
Research , vol. 57, no. 15-16, pp. 4828–4853, 2019.
[11] A. Kusiak, “Renewables: Share data on wind energy,” Nature News , vol.
529, no. 7584, p. 19, 2016.
[12] P.-N. Tan, M. Steinbach, and V . Kumar, Introduction to data mining .
Pearson Education India, 2016.
[13] F. Tao, Q. Qi, A. Liu, and A. Kusiak, “Data-driven smart manufacturing,”
Journal of Manufacturing Systems , vol. 48, pp. 157–169, 2018.
[14] L. Wang, Z. Zhang, J. Xu, and R. Liu, “Wind turbine blade breakage
monitoring with deep autoencoders,” IEEE Transactions on Smart Grid ,
vol. 9, no. 4, pp. 2824–2833, 2016.
[15] X. Wu, X. Zhu, G.-Q. Wu, and W. Ding, “Data mining with big data,”
IEEE transactions on knowledge and data engineering , vol. 26, no. 1,
pp. 97–107, 2013.
[16] J. B. Ali, L. Saidi, S. Harrath, E. Bechhoefer, and M. Benbouzid, “Online
automatic diagnosis of wind turbine bearings progressive degradations
under real experimental conditions based on unsupervised machine
learning,” Applied Acoustics , vol. 132, pp. 167–181, 2018.
[17] X. L ¨u, T. Lu, C. J. Kibert, and M. Viljanen, “Modeling and forecasting
energy consumption for heterogeneous buildings using a physical–
statistical approach,” Applied Energy , vol. 144, pp. 261–275, 2015.
[18] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” IEEE transactions on pattern analysis
and machine intelligence , vol. 35, no. 8, pp. 1798–1828, 2013.
[19] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans-
actions on knowledge and data engineering , vol. 22, no. 10, pp. 1345–
1359, 2009.
[20] B. Settles, “Active learning literature survey,” 2009.
[21] Y . Zheng, “Methodologies for cross-domain data fusion: An overview,”
IEEE transactions on big data , vol. 1, no. 1, pp. 16–34, 2015.
[22] J. Sun, H. Li, Q.-H. Huang, and K.-Y . He, “Predicting ﬁnancial distress
and corporate failure: A review from the state-of-the-art deﬁnitions,
modeling, sampling, and featuring approaches,” Knowledge-Based Sys-
tems, vol. 57, pp. 41–56, 2014.
[23] G. A. Susto, A. Schirru, S. Pampuri, S. McLoone, and A. Beghi,
“Machine learning for predictive maintenance: A multiple classiﬁer
approach,” IEEE Transactions on Industrial Informatics , vol. 11, no. 3,
pp. 812–820, 2014.
[24] J. Tautz-Weinert and S. J. Watson, “Using scada data for wind turbine
condition monitoring–a review,” IET Renewable Power Generation ,
vol. 11, no. 4, pp. 382–394, 2016.
------------------------------End of the page -----------------------------------
9
[25] W. Qiao and D. Lu, “A survey on wind turbine condition monitoring
and fault diagnosis—part i: Components and subsystems,” IEEE Trans-
actions on Industrial Electronics , vol. 62, no. 10, pp. 6536–6545, 2015.
[26] M. I. Blanco, “The economics of wind energy,” Renewable and sustain-
able energy reviews , vol. 13, no. 6-7, pp. 1372–1382, 2009.
[27] T.-P. Chang, F.-J. Liu, H.-H. Ko, S.-P. Cheng, L.-C. Sun, and S.-C. Kuo,
“Comparative analysis on power curve models of wind turbine generator
in estimating capacity factor,” Energy , vol. 73, pp. 88–95, 2014.
[28] J.-S. Chou, C.-K. Chiu, I.-K. Huang, and K.-N. Chi, “Failure analysis
of wind turbine blade under critical wind loads,” Engineering Failure
Analysis , vol. 27, pp. 99–118, 2013.
[29] M. Etemaddar, M. O. L. Hansen, and T. Moan, “Wind turbine aero-
dynamic response under atmospheric icing conditions,” Wind Energy ,
vol. 17, no. 2, pp. 241–265, 2014.
[30] I. Guyon and A. Elisseeff, “An introduction to variable and feature
selection,” Journal of machine learning research , vol. 3, no. Mar, pp.
1157–1182, 2003.
[31] E. Zdravevski, P. Lameski, V . Trajkovik, A. Kulakov, I. Chorbev, R. Gol-
eva, N. Pombo, and N. Garcia, “Improving activity recognition accuracy
in ambient-assisted living systems by automated feature engineering,”
Ieee Access , vol. 5, pp. 5262–5280, 2017.
[32] V . L ´opez, A. Fern ´andez, S. Garc ´ıa, V . Palade, and F. Herrera, “An insight
into classiﬁcation with imbalanced data: Empirical results and current
trends on using data intrinsic characteristics,” Information sciences , vol.
250, pp. 113–141, 2013.
[33] M. Tan and Z. Zhang, “Wind turbine modeling with data-driven methods
and radially uniform designs,” IEEE Transactions on Industrial Infor-
matics , vol. 12, no. 3, pp. 1261–1269, 2016.
[34] A. A. Jim ´enez, F. P. G. M ´arquez, V . B. Moraleda, and C. Q. G. Mu ˜noz,
“Linear and nonlinear features and machine learning for wind turbine
blade ice detection and diagnosis,” Renewable energy , vol. 132, pp.
1034–1048, 2019.
------------------------------End of the page -----------------------------------
